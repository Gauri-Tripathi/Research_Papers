



## Research Papaers i implemented from scratch.

| Paper Name | Description | Repo |
|------------|-------------|------|
| [Rethinking the Inception Architecture for Computer Vision] | Introduces factorized convolutions to improve efficiency and accuracy in vision models. | [GitHub](https://github.com/Gauri-Tripathi/Research_Papers/tree/main/inception) |
| [Efficient LLMs with AMP: Attention Heads and MLP Pruning]| Proposes AMP, a structured pruning method to remove less important attention heads and MLP neurons in LLMs. | [Github](https://github.com/Gauri-Tripathi/Research_Papers/tree/main/AMP) |
| [Transformers without Normalization]| Removes normalization layers using a dynamic Tanh activation while maintaining model performance. | [GitHub](https://github.com/Gauri-Tripathi/Research_Papers/tree/main/dyt) |
| [LLaMA 2] | Meta's open-weight LLMs (7Bâ€“70B), pretrained and fine-tuned for chat and instruction following. | [GitHub]() |
